\chapter{Literature review}

\section{Basic aviation terminologies}

\subsection{UAV or Drone}
UAV is short for Unmanned Aerial Vehicle, in other words an airborne vehicle that is capable of movement
without having a pilot on board. Drone is a subset of UAV, a vehicle that is capable of autonomous flight.
Usually UAV and the word Drone are used interchangeably in the literature, and it is used so in this thesis.

\subsection{Body axes used in aviation}
In aviation Euler angles are used to describe the orientation of an aircraft. Euler angles are three angles
that describe the orientation of a rigid body with respect to a fixed coordinate system. These axes are 
referred to as Yaw, Pitch and Roll. Rotation around these axes are Yaw, Pitch 
and Roll angles respectively. All three angles follow the right-hand rule. 

Magnetic north is used as reference for Yaw, so an angle of 0$^\circ$ or 360$^\circ$ means that the vehicle
is heading North, 90$^\circ$ means it's heading East. Yaw is sometimes called Heading.

\begin{figure}[!hb]
    \centering
	$\vcenter{\hbox{\includegraphics[width=90mm, keepaspectratio]{figures/plane_yaw_pitch_roll.png}}}$
    $\vcenter{\hbox{\includegraphics[width=40mm, keepaspectratio]{figures/attitude_indicator.png}}}$
    \caption{Aircraft body axes\cite{AircraftBodyAxes} and attitude instrument}
    \label{fig:aircraft_body_angles}
\end{figure}


Pitch and Roll angles are calculated in reference to the horizontal plane. The normal vector of the horizontal 
plane is used for the calculations, and it is the norm of the gravity vector. It is favorable to use the gravity
vector, because its direction can be measured using an accelerometer. Both Roll and Pitch angles are measured
from -90$^\circ$ to 90$^\circ$. A Pitch of 90$^\circ$ is straight up and 0$^\circ$ is Horizon. If an aircraft
flies with 0$^\circ$ Roll angle, the vehicle is horizontal. On the other hand a 90$^\circ$ Roll angle means
the vehicle is turning right and is perpendicular to the horizon. Pitch is sometimes referred to as Tilt.

The limitation of using Euler angles is reached when Pitch or Roll angles approach 90$^\circ$, because in this 
case one of these angles become parallel with the gravity vector and the other angle cannot be determined, 
due to lack of reference.
Euler angles and singularities are well described in \cite{diebel2006representing}. To avoid singularities
of Euler angles, Unit quaternions can be used. Quaternions are mainly used for calculations, while Euler 
angles are used to provide humanly readable values.

\subsection{Ground Control Station}
A software running on a ground computer, used for receiving in-flight information via telemetry from a UAV.
It displays status and progress of mission, that often includes sensor or video data. It can also be used
for sending commands up to the UAV during flight.

Ground Control Station is often referred to as GCS.

\subsection{Multicopters}
Multicopter or Multirotor is a generic term to describe a UAV with more than two rotors. The term covers quadcopters,
octocopters, hexicopters etc.

Quadcopters are quickly gaining popularity thanks to the easy to use, commercially available drones that can be used
for filming. Octocopters have eight rotors in total and mostly used by professional users. It provides a more stable 
flight, can lift more weight and it is safer, because it can fly with any of the rotors damaged.

\section{Simultaneous Localization And Mapping - SLAM}
Simultaneous Localization And Mapping the computational problem that asks if it is possible to place a robot 
in an unknown environment and for the robot to incrementally build a consistent map of this environment while 
simultaneously determining its position. A good overview of SLAM is presented in\cite{durrant2006simultaneous} 
and \cite{diebel2006representing} focusing a solution to the above proposed problem in general, with no dependency 
of sensors to be used.

\begin{figure}[!ht]
    \centering
	\includegraphics[width=80mm, keepaspectratio]{figures/slam_tutorial_basic_figure.png}
    \caption{The essential SLAM problem presented in \cite{durrant2006simultaneous} }
    \label{fig:slam_tutorial_basic}
\end{figure}

During SLAM process a robot is placed in an unknown location and it is capable of estimating its trajectory and 
location of all landmarks online, without the need of priori knowledge of the location. The robot makes relative
observations of its surroundings, a number of unknown landmarks using sensors as seen on figure \ref{fig:slam_tutorial_basic}.
The following quantities are defined at a time instant \emph{k}:
\begin{itemize}
	\item $\mathbf{x}_{k}$: State vector describing the location and orientation of the robot
	\item $\mathbf{u}_{k}$: Control vector, applied at \emph{k-1} and drove the robot to state $\mathbf{x}_{k}$ at time \emph{k}
	\item $\mathbf{m}_i$: Landmark location vector. A time independent vector that describes the location of a static landmark
	\item $\mathbf{z}_{ik}$: Observation taken from the robot to the \emph{i}th landmark at time \emph{k}
	\item $\mathbf{X}_{0:k}=\left \{ \mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_k \right \}$ A set of all robot location vectors until time \emph{k}
	\item $\mathbf{U}_{0:k}=\left \{ \mathbf{u}_1,\mathbf{u}_2,\cdots,\mathbf{u}_k \right \}$ A set of history of all control vectors until time \emph{k}
	\item $\mathbf{m}=\left \{ \mathbf{m}_1,\mathbf{m}_2,\cdots,\mathbf{m}_n \right \}$ A set of all landmarks
	\item $\mathbf{Z}_{0:k}=\left \{ \mathbf{z}_1,\mathbf{z}_2,\cdots,\mathbf{z}_k \right \}$: A set of all landmark observations until time \emph{k}
\end{itemize}

In probabilistic form of SLAM, the probability distribution function \ref{eq:slam_probability} needs to be computed for
all times \emph{k}. In other words the current state vector and the landmark location probabilities need to be calculated
based on all \emph{k} observations, control vectors and the initial state of the robot.

\begin{equation} \label{eq:slam_probability}
    P\left ( \mathbf{x}_{k},\mathbf{m}\mid \mathbf{Z}_{0:k},\mathbf{U}_{0:k},\mathbf{x}_{0}\right )
\end{equation}

A recursive solution is desirable for the SLAM problem. Starting with an estimate for the posteriori distribution 
(equation \ref{eq:slam_probability_posteriori}) at time \emph{k}-1, then a control vector $\mathbf{u}_{k}$ is used to
estimate the next state and observations using Bayes theorem. This calculation requires an observation model and a 
state transition model.
 


\begin{equation} \label{eq:slam_probability_posteriori}
    P\left ( \mathbf{x}_{k-1},\mathbf{m}\mid \mathbf{Z}_{0:k-1},\mathbf{U}_{0:k-1},\mathbf{x}_{0}\right )
\end{equation}

The observation model \ref{eq:slam_observation_model} describes the probability of making an observation 
$\mathbf{z}_{k}$ given the current robot and landmark locations.

\begin{equation} \label{eq:slam_observation_model}
    P\left ( \mathbf{z}_{k}\mid \mathbf{x}_{k},\mathbf{m} \right )
\end{equation}

The state transition model \ref{eq:slam_motion_model} describes the probability of the next position of the robot 
based on the previous state and the control vector at time \emph{k}. The state transition is assumed to be a Markov
process, so the next state $\mathbf{x}_{k}$ depends only on the current state $\mathbf{x}_{k-1}$ applied control 
$\mathbf{u}_{k}$. $\mathbf{x}_{k}$ is also independent from both landmark locations $\mathbf{m}$ and observations
$\mathbf{z}_{k}$.

\begin{equation} \label{eq:slam_motion_model}
    P\left ( \mathbf{x}_{k}\mid \mathbf{x}_{k-1},\mathbf{u}_{k} \right )
\end{equation}

Using equations \ref{eq:slam_probability_posteriori}, \ref{eq:slam_observation_model} and \ref{eq:slam_motion_model},
the SLAM algorithm can now be described using a two-step recursive form: prediction and correction.
The prediction step or time-update is used for estimating the next state and map, based on all posteriori observations,
control vectors and initial state. 

\begin{equation} \label{eq:slam_time_update}
    P\left ( \mathbf{x}_{k}, \mathbf{m}\mid \mathbf{Z}_{0:k-1}, \mathbf{U}_{0:k}, \mathbf{x}_{0} \right )=
        \int 
            P\left ( \mathbf{x}_{k}\mid \mathbf{x}_{k-1}, \mathbf{u}_{k} \right ) 
            \times  
            P\left ( \mathbf{x}_{k-1}, \mathbf{m}  \mid  \mathbf{Z}_{0:k-1}, \mathbf{U}_{0:k-1}, \mathbf{x}_{0}\right )
        d\mathbf{x}_{k-1}
\end{equation}

The measurement update provides a correction to the prediction state, using observations made at time \emph{k}.

\begin{equation} \label{eq:slam_mesurement_update}
    P\left ( \mathbf{x}_{k}, \mathbf{m} \mid \mathbf{Z}_{0:k}, \mathbf{U}_{0:k}, \mathbf{x}_{0} \right )= 
    \frac
        {P\left ( \mathbf{z}_{k}\mid \mathbf{x}_{k}, \mathbf{m} \right )P\left ( \mathbf{x}_{k}, \mathbf{m}\mid \mathbf{Z}_{0:k-1}, \mathbf{U}_{0:k}, \mathbf{x}_{0}\right ) }
        {P\left ( \mathbf{z}_{k} \mid \mathbf{Z}_{0:k-1}, \mathbf{U}_{0:k}\right )}
\end{equation}


The localization problem can be solved with the assumption the map is known:

\begin{equation} \label{eq:slam_localization_problem}
    P\left ( \mathbf{x}_{k} \mid \mathbf{Z}_{0:k}, \mathbf{U}_{0:k}, \mathbf{m}\right )
\end{equation}

And the mapping problem can be solved with the assumption the location is known:

\begin{equation} \label{eq:slam_mapping_problem}
    P\left ( \mathbf{m} \mid \mathbf{X}_{0:k}, \mathbf{Z}_{0:k}, \mathbf{U}_{0:k}, \right )
\end{equation}

These problems introduced in \ref{eq:slam_localization_problem} and \ref{eq:slam_mapping_problem} are
dependent on each other and need to be solves simultaneously. In the early stages of SLAM mapping and localization
were tried to be solved independently and work often focused on either of them. Once the realization came that combined 
mapping and localization can be formulated as a single estimation problem, the problem became convergent.

Many solutions can be found for probabilistic SLAM problem, the most common is the use of the extended Kalman
filter (EKF) that provides an optimal estimation to non-linear models with additive Gaussian noise. The motion
model can also be represented with a non-Gaussian probability distribution, that leads to the use of the 
Rao-Blackwellized particle filter, or FastSLAM algorithms.


The above introduced SLAM problem was a general introduction, it is not dependent on the sensor used for observations.
Many kind of observers can be used for this purpose, but the two main categories are visual or camera based and sensor based.
Visual observers mostly use simple or special camera configurations for depth sensing, while sensor based observers 
use some other technique or physical phenomena to measure distances like sonars or LIDARS.

\subsection{Visual SLAM}
Visual SLAM uses a simple camera or a modified camera that is capable of depth sensing. This technique is 
similar to how humans and animals perceive and their surroundings. 

In solutions where a single camera is used, called monocular camera, depth sensing is done by using the differences
between consecutive images. 
Two main categories of monocular techniques are feature-based and direct SLAM algorithms. Feature-based techniques detect
certain key-points called features in images, like corners and edges, and only uses these feature to extract depth information.
Direct SLAM algorithms use the image intensities to estimate the location and surroundings.
LSD-SLAM proposed in \cite{engel2014lsd} is a good example for feature-based, while ORB-SLAM\cite{mur2015orb} uses direct SLAM 
approach.

The pro of using monocular camera for localization and mapping is that each image contains high density of information and
cameras are cheap and popular.
The drawback of using monocular camera based SLAM is that a single camera cannot detect the scale. This is referred to as
scale-drift and often attempted to be fixed by trying to detect a scenery that has been previously scanned and therefore 
drift can be corrected.

Stereo camera or RGB-D cameras are also popular for Visual SLAM, these avoid the scale-drift that monocular cameras suffer
from \cite{engel2015large}. 

\subsection{LIDAR SLAM}
Cameras are cheap and easily available, but they produce high amounts of data that demands high processing capabilities.
Sensor based approaches produce lower amount of data that already contains depth information without the need for further 
processing. 

Ultrasonic sensors are easy to use for distance measuring, they are cheap and easy to use, but they suffer from significant
measurement noise, because of background noise and the change of speed of sound. LIDAR sensors on the other hand 
use light to measure distance, offer less noisy measurements, lower form factor, higher resolution and update rates. LIDAR 
sensors are also significantly more expensive than most of ultrasonic sensors and camera based SLAM systems.


The study of University of California\cite{hening20173d} presents a data fusion algorithm for estimation of velocity and 
position of a UAV in urban environments where GPS data is partly or completely blocked and is 
unreliable. A LIDAR sensor provides local position updates using SLAM technique, GPS provides 
corrections when available and an Internal Navigation System is used as an additional input to
the Adaptive Extended Kalman filter. The outline of the filter can be seen on figure \ref{fig:haning_filter}

\begin{figure}[!hb]
    \centering
	\includegraphics[width=100mm, keepaspectratio]{figures/hening_filter.png}
    \caption{Schematic of LIDAR SLAM, GPS and IMU integration \cite{hening20173d}}
    \label{fig:haning_filter}
\end{figure}

The LIDAR used for measurements is a Velodyne VLP-16, that has a vertical field of view of 30$^\circ$ with a 
vertical resolution of 2$^\circ$. On the horizontal axis it's resolution is 0.1-0.4$^\circ$ and 
rotation rate can be adjusted between 5-20Hz.

The proposed filter is was capable of reducing the GPS drift of 24.3m and LIDAR drift of 7.5m to 3.42m.
This shows that fusing GPS, LIDAR and IMU measurements results in a more accurate position estimate in
GPS-degraded environments. The capabilities of the LIDAR was tested on an even moon like surface, 
with very few objects that can be used by the SLAM algorithm. In this case GPS data is much more 
accurate, than LIDAR SLAM results.

\begin{figure}[!ht]
    \centering
	$\vcenter{\hbox{\includegraphics[width=80mm, keepaspectratio]{figures/hening_LIDARvsGPS.png}}}$
	$\vcenter{\hbox{\includegraphics[width=50mm, keepaspectratio]{figures/hening_result.png}}}$
    \caption{GPS, LIDAR and EKF position estimates and position drift after 405 meters\cite{hening20173d}}
    \label{fig:lidar_slam_integration}
\end{figure}


\subsection{Cartographer SLAM}
Cartographer is a system developed by Google that provides real-time simultaneous localization and mapping
in 2D and 3D across multiple platforms and sensor configurations. Cartographer is used by Google street view
for internal mapping of building interiors, using a backpack mounted LIDAR scanner.

There officially available SLAM packages in ROS, the two most popular packages are Gmapping and Cartographer.
Gmapping package contains a ROS wrapper for OpenSlam's Gmapping that provides laser based SLAM as a ROS node. 
Gmapping is limited to 2D map building, therefore it is not considered for this project.

\begin{figure}[!ht]
    \centering
	\includegraphics[width=140mm, keepaspectratio]{figures/cartographer_example.png}
    \caption{Cartographer SLAM example\cite{CartographerDocumentation}}
    \label{fig:cartographer_slam_example}
\end{figure}


On the other hand Cartographer ROS package supports both 2D and 3D SLAM and has a great and active community.
The overview of Cartographer is seen on figure \ref{fig:cartographer_slam_overview}. The core components are
sensor inputs, Local SLAM and Global SLAM. On a higher abstraction the job of local SLAM is to build submaps
and the job of global SLAM is to tie submaps the most consistently together.
It is common to build submaps or local maps in SLAM implementations, because building a single map increases 
complexity quadratically, but by using submaps the complexity can be limited to the contents of the submap.

The most important input of Cartographer is the Range Data, depth information coming from LIDAR sensors. 
The data is first pre-filtered, because some measurements are irrelevant the sensor might be directed to
a part of the robot or it can be covered by dust. Some sensors set unsuccessful range measurements to a value
that is significantly higher than the maximum range of the sensor. A bandpass filter is used for pre-filtering,
that keeps the values in a predefined range. The minimum and maximum values need to be set according to 
the sensor specifications. Close objects are very often hit by the LIDAR
measurements and offer more points, however distant objects offer much less points. The Voxel filter 
downsamples raw points where density is higher to reduce computation needed.

Inertial Measurement Units (IMU) provide a direction of gravity vector and a noisy estimation of the robot's
rotation. IMU data has to be provided for 3D SLAM because it greatly reduces complexity of scan matching,
while IMU data is optional for 2D SLAM. Odometry Pose and Fixed Frame Pose are optional inputs of 
Cartographer, that can further reduce complexity.


\begin{figure}[!hb]
    \centering
	\includegraphics[width=140mm, keepaspectratio]{figures/cartographer_slam.png}
    \caption{Cartographer SLAM overview\cite{CartographerDocumentation}}
    \label{fig:cartographer_slam_overview}
\end{figure}



\section{Similar products available on the market} 

\subsection{Terabee TeraRanger Tower} \label{sect:TerabeeDescription}
TeraBee offers an off-the-shelf solid-state LIDAR system with the purpose of collision avoidance for drones.
In their solution 8 sensors are evenly distributed around the vertical axis with a controller board in 
the middle. TeraRanger's interface is compatible with Pixhawk 4 flight controller board and 
PX4 flight controller software, that makes integration easy into systems based on these.

\begin{figure}[!ht]
    \centering
    $\vcenter{\hbox{\includegraphics[width=80mm, keepaspectratio]{figures/tera_ranger_tower.png}}}$
    $\vcenter{\hbox{\includegraphics[width=50mm, keepaspectratio]{figures/tera_ranger_tower_2.png}}}$
    \caption{Terabee TeraRanger Tower Evo dimensions}
    \label{fig:teraranger_dimensions}
\end{figure}

Each block of the array is a standalone LIDAR sensor, that can be used separately and supports different
mount configurations. The company offers a long-range and fast-ranging version of these sensors, depending 
on the type an update rate of 320 Hz can be achieved. The fact that system comes with a ROS package and a 
2$^{\circ}$ field of view makes it a potential candidate for indoor mapping and positioning in two dimensions. 
The price of this setup starts from 599 euro\cite{TerabeeTeraRanger}.

\begin{table}[ht]
	\footnotesize
	\centering
	\begin{tabular}{ l c c }
		\toprule
		                & Long-range                                & Fast-ranging \\
		\midrule
		Range           & 0.5m up to 60m                            & 0.75m up to 8m \\
		Update rate     & 120Hz/sensor                              & 320Hz/sensor\\
		Field of View   & 2$^{\circ}$                               & 2$^{\circ}$\\
		Accuracy        & $\pm 4cm$ in the first 14m, 1.5\% above   & $\pm 12cm$\\
		\bottomrule
	\end{tabular}
	\caption{TeraRanger Tower Evo specifications}
	\label{tab:tera_ranger_features}
\end{table}

Terabee provides a tutorial video on their website\cite{TerabeeTeraRanger} how to connect the 
sensor array to a UAV that uses Pixhawk 4 flight controller and the process of configuration 
in two different GCS programs. I have learned that PX4 flight stack supports lidar measurements
for obstacle avoidance and it can be configured from a GCS program. It seems a reasonable choice 
to integrate this feature into such product.

\subsection{Crazyflie Multi-ranger deck}
The company Bitcraze has developed a mini quadcopter mainly for educational purposes. The current version is
called Crazyflie 2.1 and measures only 92x92mm with a height of 29mm and a weighs 27g.
Extra sensors and peripherals can be attached to the top of the quadcopter using extension boards.

The extension board called Multi-ranger deck has 5 VL53L1X sensors by STMicroelectronics facing forwards, 
backwards, left, right and up. This project is similar to the product of Terabee described in \ref{sect:TerabeeDescription}, 
but in a smaller size factor and with significantly lower weight.

An introduction video can be found on the product website \cite{BitcrazeMultirangerDeck}, where 
a SLAM algorithm is used to create a map and localize the drone. This serves as a proof of concept,
that static VL53L1X sensors can be used for mapping and positioning in two dimensions. The company provided 
no information of the SLAM algorithm used or from the quality of the map.

\begin{figure}[!ht]
    \centering
    $\vcenter{\hbox{\includegraphics[width=60mm, keepaspectratio]{figures/multiranger_deck.jpg}}}$
    $\vcenter{\hbox{\includegraphics[width=80mm, keepaspectratio]{figures/multiranger_slam.png}}}$
    \caption{Crazyflie Multi-ranger deck, SLAM example project}
    \label{fig:crazyflie_multiranger}
\end{figure}

